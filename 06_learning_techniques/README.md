
# 6장 학습 관련 기술들
> 신경망 학습의 학습 효율과 정확도를 높이기 위한 다양한 기법들에 대해 이해하는 것
> 학습효율 : 매개변수갱신(SGD, Momentum, AdaGrad, Adam) 배치 정규화(Batch Normalization)
> 정확도 : 가중치 초기화(Xavier, He), 배치 정규화(Batch Normalization), 가중치 감소(Weight Decay), 드롭아웃(Dropout)


## 6.1 매개변수 갱신 방법의 종류 및 이해
> 우리가 얻고자 하는 바는 매개변수를 찾는 것인데, 매개변수 공간은 충분히 넓어서 무작정 찾는 방법보다 '똑똑한' 접근이 필요하며, 이를 위해 다양한 '최적화 기법'을 이해하고 적용해 본다.

### 확률적 경사하강법 (SGD, Stochastic Gradient Descent)
> 일반적인 경사하강법과 같은 방법이지만, 대게 신경망 학습은 미니배치를 통한 반복적인 학습을 거치게 되고 미니배치 자체가 랜덤 샘플링을 하게 되므로 확률적 경사하강법이라 부른다. 즉, Mini Batch Gradient Descent 라고 부르기도 하며, 이러한 MGD 작업을 반복하게 되므로 수 많은 시도를 통해 확률적으로 더 나은 매개변수를 선택한다는 의미로 해석하였다
* SGD는 단순하고 구현도 쉬운 것이 장점이지만, 해결해야 하는 문제에 따라 아주 비효율적일 수 있습니다. 즉 비등방성 anisotropy 함수(방향에 따라 성질, 기울기 등이 달라지는 함수, saddle)에서는 탐색 경로가 아주 비효율적일 수 있어 Momentum, AdaGrad, Adam 등의 개선된 최적화 기법을 사용할 수 있습니다.

### 모멘텀 (Momentum)
> '운동량'을 뜻하는 단어로 움직이던 방향의 가속도를 의미하며, 움직이던 대로 계속 움직이려는 힘을 말합니다. 즉 SGD와 같이 지그재그로 움직이기 보다는 한 번 결정된 방향대로 움직이려는 경향을 가지고 있어서 경로를 찾을 때에 쓸데 없이 많이 움직이는 것을 피할 수 있습니다.

### AdaGrad (Adaptive Gradient)
> 신경망 학습에는 학습률을 정하는 효과적인 기술로 **학습률 감소 learning rate decay**가 있는데 이는 초기에는 정확히 모르기 때문에 과감하게 움직이고 어느 정도 수렴하는 시점에서는 조심스럽게 움직인다고 생각하면 되는데, 조금씩 조금씩 학습률을 줄여가는 기법이다. 말 그대로 적응해(adaptive) 가면서 학습을 한다는 의미이다. 그래서 모멘텀 보다 더 불필요한 움직임을 줄일 수 있어 더 빨리 수렴한다.

### RMSProp (Root Mean Square Propagation)
> AdaGrad와 유사한 방법이지만, 학습률 감소방법을 **지수이동평균 exponential moving average, EMA** 기법을 적용한 점이 다르다. 즉, 모든 기울기를 균일하게 더하는 것이 아니라, 먼 기울기는 서서히 잊고, 새로운 기울기를 크게 반영한다는 의미이다

### Adam
> 모멘텀은 부드럽게 수렴하고, AdaGrad(RMSProp)는 다소 경직되어 있는데 이 두 가지 방식을 합한 것이 아담이며 가장 많이 사용되는 방법이기도 하다.


## 6.2 가중치 초기값의 중요성과 접근방법
> 결국에는 목적이 가중치 확률변수를 전반적으로 퍼뜨리기 위해서이며, 초기화 값은 초반에만 좋은 효과와 분포를 가지는데 정규화(whitening)하는 것은 매번 중요한 것을 찾아낼 수 있기 때문이다
> iteration 마다 scale + shift 값을 계속 저장해두고, 테스트 시에는 전체 평균에 대한 값을 가지고 있고 지수평균이동을 활용한다 

### Xavier 초깃값

### He 초깃값


## 6.3 배치 정규화
> 배치정규화를 통해 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 되는 장점이 있다

## 6.4 오버피팅을 억제하는 기술

### 가중치 감소

### 드롭아웃
> 어려운 문제를 해결하기 위해서는 깊고 복잡한 노드를 통해 학습이 필요하고, 오버피팅은 피해야 하므로 노드 수를 줄일 필요가 있다.
> 한 편으로 Iteration을 거듭할 때마다 조금씩 다른 모델을 만들게 되어 조금 더 일반화 한 모델을 만들어내는 효과도 있다. (앙상블)


## 6.5 하이퍼파라미터의 탐색
> 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다


