
# 6장 학습 관련 기술들
> 신경망 학습의 학습 효율과 정확도를 높이기 위한 다양한 기법들에 대해 이해하는 것
* 학습효율 : 매개변수갱신(SGD, Momentum, AdaGrad, Adam) 배치 정규화(Batch Normalization)
* 정확도 : 가중치 초기화(Xavier, He), 배치 정규화(Batch Normalization), 가중치 감소(Weight Decay), 드롭아웃(Dropout)


## 6.1 매개변수 갱신 방법의 종류 및 이해
> 우리가 얻고자 하는 바는 매개변수를 찾는 것인데, 매개변수 공간은 충분히 넓어서 무작정 찾는 방법보다 '똑똑한' 접근이 필요하며, 이를 위해 다양한 '최적화 기법'을 이해하고 적용해 본다.

### 확률적 경사하강법 (SGD, Stochastic Gradient Descent)
> 일반적인 경사하강법과 같은 방법이지만, 대게 신경망 학습은 미니배치를 통한 반복적인 학습을 거치게 되고 미니배치 자체가 랜덤 샘플링을 하게 되므로 확률적 경사하강법이라 부른다. Mini Batch Gradient Descent 라고 부르기도 하며, 이러한 MGD 작업을 반복하게 되므로 수 많은 시도를 통해 확률적으로 더 나은 매개변수를 선택한다는 의미로 해석하였다
* SGD는 단순하고 구현도 쉬운 것이 장점이지만, 해결해야 하는 문제에 따라 아주 비효율적일 수 있습니다. 즉 비등방성 anisotropy 함수(방향에 따라 성질, 기울기 등이 달라지는 함수, saddle)에서는 탐색 경로가 아주 비효율적일 수 있어 Momentum, AdaGrad, Adam 등의 개선된 최적화 기법을 사용할 수 있습니다.

### 모멘텀 (Momentum)
> '운동량'을 뜻하는 단어로 움직이던 방향의 가속도를 의미하며, 움직이던 대로 계속 움직이려는 힘을 말합니다. 즉 SGD와 같이 지그재그로 움직이기 보다는 한 번 결정된 방향대로 움직이려는 경향을 가지고 있어서 경로를 찾을 때에 쓸데 없이 많이 움직이는 것을 피할 수 있습니다.

### AdaGrad (Adaptive Gradient)
> 신경망 학습에는 학습률을 정하는 효과적인 기술로 **학습률 감소 learning rate decay**가 있는데 이는 초기에는 정확히 모르기 때문에 과감하게 움직이고 어느 정도 수렴하는 시점에서는 조심스럽게 움직인다고 생각하면 되는데, 조금씩 조금씩 학습률을 줄여가는 기법이다. 말 그대로 적응해(adaptive) 가면서 학습을 한다는 의미이다. 그래서 모멘텀 보다 더 불필요한 움직임을 줄일 수 있어 더 빨리 수렴한다.

### RMSProp (Root Mean Square Propagation)
> AdaGrad와 유사한 방법이지만, 학습률 감소방법을 **[지수이동평균 exponential moving average, EMA](http://research.mrlatte.net/2016/10/emaexponential-moving-average.html)** 기법을 적용한 점이 다르다. 즉, 모든 기울기를 균일하게 더하는 것이 아니라, 먼 기울기는 서서히 잊고, 새로운 기울기를 크게 반영한다는 의미이다

### Adam
> 모멘텀은 부드럽게 수렴하고, AdaGrad(RMSProp)는 다소 경직되어 있는데 이 두 가지 방식을 합한 것이 아담이며 가장 많이 사용되는 방법이기도 하다.


## 6.2 가중치 초기값의 중요성과 접근방법
> 최종목적이 가중치 확률변수를 전반적으로 퍼뜨려서, 학습이 잘 되게 하기 위함이다
* 초깃값을 0으로 두는 경우
    * 역전파 시에 동일한 값으로 업데이트 되므로, 오히려 무작위 값이 설정되어야만 한다
* 초깃값 업데이트
    * 정규화(whitening)를 통해 매번 초기화 효과를 유지시킬 수가 있다
    * 정규화 과정에서 원래 정보가 소실될 수 있으므로, 매 iteration 마다 scale + shift 값을 저장하고, 테스트 시에 전체 평균에 대한 값을 가지고 있고 지수평균이동을 활용한다 
* 표준편차가 1인 정규분포를 이용한 초깃값
    * Sigmoid 함수의 특성상, 0과 1에 가까운 값들이 대부분이므로 각 층의 활성화 값의 분포가 0과 1에 치우치는 문제가 발생한다
    * **0과 1에 치우치는 경우 기울기가 0이 되기 때문**에 학습이 제대로 이루어지지 않는다.
* 표준편차가 0.01인 정규분포를 이용한 초깃값
    * 이는 Sigmoid 함수의 중앙인 0.5를 기준으로 몰려있는 현상이 발생한다.
    * 이 경우는 **0.5에 몰려 있기 때문에 표현력의 제한**이 있다

### Xavier 초깃값
> 각 층의 활성화값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾으려고 하였음
* 표준편차의 값을 1/sqrt(n)인 분포를 사용하면 좋다고 한다. (n값은 앞 층의 노드 수)
    * 사비에르 초깃값에 의해 레이어의 분포가 광범위하게 분포 되었으나, 층이 깊어질 수록 모양이 깨어진다
    * 사비에르 초깃값은 활성화 함수가 선형인 것을 전제로 하였기 때문에 활성화 함수의 중앙 부근이 선형인 Sigmoid, Tanh 적합하다

### He 초깃값
> ReLU의 경우는 0이하의 위치에서는 모두 0값이므로 N값도 절반으로 생각하여 sqrt(2/n) 인 정규분포를 사용한다
* 초깃값에 따른 활성화 함수들의 특성
    * ReLU 경우 한 쪽으로 치우치며 깊어질 수록 0에 가까운 값의 분포
    * Xavier 경우 초기에는 넓게 분포되나, 깊어질 수록 0에 치우지는 분포 
    * He 경우, 레벨이 깊어져도 대부분 넓게 분포되는 특성이 가져서 역전파에도 적절한 값이 전달될 수 있을 것으로 예상됨


## 6.3 배치 정규화
> 배치정규화를 통해 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 되는 장점이 있다

## 6.4 오버피팅을 억제하는 기술

### 가중치 감소

### 드롭아웃
> 어려운 문제를 해결하기 위해서는 깊고 복잡한 노드를 통해 학습이 필요하고, 오버피팅은 피해야 하므로 노드 수를 줄일 필요가 있다.
> 한 편으로 Iteration을 거듭할 때마다 조금씩 다른 모델을 만들게 되어 조금 더 일반화 한 모델을 만들어내는 효과도 있다. (앙상블)


## 6.5 하이퍼파라미터의 탐색
> 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다


