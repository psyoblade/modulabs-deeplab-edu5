{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아이온 이탈 유저 예측\n",
    "> 클래스를 분류하기 보다는 실제 회귀식을 구하는 것이 목적이나, 일반 분류기로는 예측하기는 어렵기 때문에 클래스 구분으로 일단 한 번 더 시도해보기로 함\n",
    "\n",
    "### 1. 클래스 맵핑\n",
    "> 0 - 5 사이의 값은 0, 6 ~ 15 사이 값은 5, ... , 55+ 이상은 55 으로 클래스 맵핑\n",
    "> 구글 닥스에 업로드 후 vslookup 통하여 관련 전처리 수행하고 csv 로 다운로드\n",
    "\n",
    "### 2. 추가 특질\n",
    "> id, week 정보로 sort 한 이후에 접속 횟수를 특질로 추가\n",
    "> 맥 로컬에서 sort 후 저장\n",
    "\n",
    "### 3.  선형 회귀 기울기를 4주차 간 수행하여 특질로 변경\n",
    "> 정렬된 데이터를 기준으로 세션처리를 통해서 특질(최대접속주차, 최대 4주간의 기울기를 특질 별로 생성)\n",
    "> linear regression 통해서 각 지표의 4주 간의 X = { 1, 2, 3, 4 } Y = { y1, y2, y3, y4 } 기울기를 특질로 추가\n",
    "\n",
    "### 4. 1차 실험 결론\n",
    "> Decision Tree 및 기타 Categorical 분류를 통해 실험해 보았으나, 초기 Session 처리 전에는 70% 정도 정확률을 보였고, 이는 실제 Contest 상에서는 1.4 정도의 오류율을 보였다\n",
    "> 이에 Session 처리 후에 학습 및 Contest 시도를 해 보았으나, 실험 정확률도 70% -> 60% 로 떨어졌으며, 중복 세션의 정확률에 기여하는 바도 클 것이라 생각해서 시도해 보았으나, Contest 상에서도 근소하지만 다소 오류율은 높아졌다\n",
    "> 마지막으로 Class 결과를 Numeric 으로 변경하고 MultilayerPerceptron 으로 학습하였고 결과가 다소 상이하지만 실험 결과를 Contest 올렸고 오류율이 1.4 -> 1.1 까지 떨어져서 기분이 좋았다.\n",
    "> 현재 실험 결과는 /Users/psyoblade/aion/20180609 폴더에 백업하고 다시 /Users/psyoblade/aion/data 폴더에서 실험 시작\n",
    "\n",
    "### 5. 향후 방향\n",
    "> 다음 실험은 원본 데이터에도 가공하지 않고 정답을 그대로 사용하고, 학습 및 분류해보는 것이고, 그 다음 스텝은 텐서플로우나 딥러닝 기법을 응용해 보는 것이 도전 과제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'survival'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68e53fe96cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/train_data_sorted.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/train_data_output.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m                \u001b[0;31m# 테스트용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/test_data_sorted.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68e53fe96cbb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(source_data, target_data, threshold, training)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcurr_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actor_account_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprev_id\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprev_id\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcurr_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mx_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessionize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mx_sessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68e53fe96cbb>\u001b[0m in \u001b[0;36msessionize\u001b[0;34m(sessions, training)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# 기존의 값들은 그대로 사용하는 것이 좋을 것 같고 증감에 유의미한 필드에 대해서만 선정해서 필드를 추가하도록 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msessionize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mx_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_sessions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_sessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0mx_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mx_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68e53fe96cbb>\u001b[0m in \u001b[0;36maverage_sessions\u001b[0;34m(sessions, keys_sessions, training)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_survival\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survival\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_survival_time\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survival_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'survival'"
     ]
    }
   ],
   "source": [
    "# -- 세션처리 통해서 actor_account_id 당 4주의 데이터를 묶는 작업  -- 25개 필드\n",
    "#!/usr/bin/env python\n",
    "# -*- coding:utf -*-\n",
    "import sys, csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일을 읽어서 Dictionary 형태의 List 로 반환하는 함수\n",
    "def dictionize(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = [r for r in reader]\n",
    "    return data\n",
    "\n",
    "# 파일을 읽어서 List of List 형태로 반환하는 함수\n",
    "def listinize(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        data = [r for r in reader]\n",
    "    return data\n",
    "\n",
    "\"\"\" \n",
    "-- 사전 정의된 (23개)\n",
    "actor_account_id : 계정 ID\n",
    "combine_cnt : 제작 횟수\n",
    "dice_cnt : 주사위 횟수\n",
    "enter_dd_cnt : 주간 접속 일자 수\n",
    "exp_get : 경험치 획득량\n",
    "fortress_cnt : 요새전 보상 횟수\n",
    "get_ap :AP 획득량\n",
    "get_gp : GP 획득량\n",
    "glide_cnt : 활강 횟수\n",
    "harvest_cnt : 채집 횟수\n",
    "inc_kina_sum :키나 획득량 (개인간 거래 포함)\n",
    "indun_cnt : 인스턴스 던전 입장 횟수\n",
    "kina_sys_inc : 개인간 거래를 제외한 키나 증가량 (e.g. 사냥, 퀘스트 등으로 획득한 키나)\n",
    "kina_sys_dec : 개인간 거래를 제외한 키나 감소량 (e.g. NPC 구매, 텔레포트 등으로 사용한 키나)\n",
    "npc_sell_kinasum : 상점 판매로 획득한 키나량\n",
    "pay_amt_total : 한 주간 구매한 금액 (주별로 표준화 되어있음)\n",
    "pvp_cnt : PvP 횟수\n",
    "playtime_ss : 플레이타임(초)\n",
    "pve_cnt : NPC 킬 횟수\n",
    "quest_cnt : 퀘스트 완료 횟수\n",
    "teleport_cnt : 텔레포트 횟수\n",
    "use_scroll_cnt : 주문서 사용 횟수\n",
    "week : 플레이한 주 차 ( 계정별 최대 4주 )\n",
    "\n",
    "-- 추가 지표 증가량 lm.coef_ (13개))\n",
    "\n",
    "x_combine_cnt\n",
    "x_enter_dd_cnt\n",
    "x_exp_get\n",
    "x_fortress_cnt\n",
    "x_get_ap\n",
    "x_get_gp\n",
    "x_harvest_cnt\n",
    "x_inc_kina_sum\n",
    "x_playtime_ss\n",
    "x_pve_cnt\n",
    "x_pvp_cnt\n",
    "x_quest_cnt\n",
    "x_use_scroll_cnt\n",
    "\n",
    "-- 정답 label (2개)\n",
    "z_survival_time : 유지 일자 \n",
    "z_survival : 유지 그룹 라벨 (c_0, c_5, c_15, c_25, c_35, c_45, c_55) 총 6개 그룹으로 정의\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 입력받은 튜플들의 각 값들의  주어진 킷값들에 대해서 평균치를 저장하여 하나의 dict 로 반환\n",
    "#\n",
    "# arguements sessions as list[dict]\n",
    "# returns dict\n",
    "#\n",
    "def average_sessions(sessions, keys_sessions, training=False):\n",
    "    average = sessions[0]\n",
    "    average[\"week\"] = 0\n",
    "    if training:\n",
    "        average[\"y_survival\"] = average.pop(\"survival\")\n",
    "        average[\"y_survival_time\"] = average.pop(\"survival_time\")\n",
    "    else:\n",
    "        average[\"y_survival\"] = \"?\"\n",
    "        average[\"y_survival_time\"] = \"?\"\n",
    "    \n",
    "    num_of_sessions = len(sessions)\n",
    "    if num_of_sessions == 1:\n",
    "        average[\"week\"] = 1\n",
    "        return average\n",
    "    \n",
    "    for session in sessions:\n",
    "        for key in keys_sessions:\n",
    "            value = session.get(key)\n",
    "            if key == \"actor_account_id\":\n",
    "                average[key] = value\n",
    "            elif key == \"week\":\n",
    "                average[key] = int(average.get(\"week\", 0)) + 1\n",
    "            else:\n",
    "                average[key] = float(average.get(key, 0.0)) + float(value)\n",
    "                \n",
    "    for key in keys_sessions:\n",
    "        if key != \"actor_account_id\" and key != \"week\":\n",
    "            average[key] = float(average.get(key)) / num_of_sessions\n",
    "            \n",
    "    return average\n",
    "\n",
    "# 입력 받은 킷값에 대하여 주어진 튜플들의 선형 회귀 직선의 기울기를 추가하여 x_${key} 값으로 dict 반환\n",
    "# 단 튜플 수가 1개이면 모든 값은 0.0으로 반환\n",
    "def extract_coef(sessions, keys):\n",
    "    coef = {}\n",
    "    num_of_sessions = len(sessions)\n",
    "    # 튜플 수가 1개인 경우만 먼저 적용\n",
    "    if len(sessions) == 1:\n",
    "        coef = { \"x_\" + key:0.0 for key in keys }\n",
    "    else:\n",
    "        X = np.asarray(list(range(num_of_sessions)), dtype=np.float)\n",
    "        X_train = X.reshape(num_of_sessions, 1)\n",
    "        regressor = LinearRegression()\n",
    "        for key in keys:\n",
    "            Y = np.asarray([ sessions[x][key] for x in range(num_of_sessions) ], dtype=np.float)\n",
    "#             Y = preprocessing.normalize(Y, norm='l2') # normalize 하면 수치가 너무 작아져서 coef 값이 변별력이 떨어짐.\n",
    "            y_train = Y.reshape(num_of_sessions, 1)\n",
    "            regressor.fit(X_train, y_train)\n",
    "            coef[\"x_\" + key] = regressor.coef_[0][0]# 기울기는 coef 이고, 절편이 coef 이다.!\n",
    "    return coef\n",
    "\n",
    "keys_sessions = [ \"actor_account_id\", \"combine_cnt\", \"dice_cnt\", \"enter_dd_cnt\", \"exp_get\"\n",
    "                 , \"fortress_cnt\" , \"get_ap\", \"get_gp\", \"glide_cnt\", \"harvest_cnt\"\n",
    "                 , \"inc_kina_sum\" ,  \"indun_cnt\", \"kina_sys_inc\", \"kina_sys_dec\", \"npc_sell_kinasum\"\n",
    "                 , \"pay_amt_total\" , \"pvp_cnt\", \"playtime_ss\", \"pve_cnt\", \"quest_cnt\"\n",
    "                 , \"teleport_cnt\" , \"use_scroll_cnt\", \"week\" ] # 23개 (0~22)\n",
    "keys_coef = [ \"combine_cnt\", \"enter_dd_cnt\", \"exp_get\", \"fortress_cnt\", \"get_ap\"\n",
    "                   , \"get_gp\", \"harvest_cnt\", \"inc_kina_sum\", \"playtime_ss\", \"pve_cnt\" \n",
    "                   , \"pvp_cnt\", \"quest_cnt\", \"use_scroll_cnt\" ] # 13개 (23~35)\n",
    "keys_labels = [ \"survival\" ] # 1개 (36)\n",
    "\n",
    "SIZE_OF_CORE_KEYS = len(keys_sessions)\n",
    "SIZE_OF_COEF_KEYS = len(keys_coef) + SIZE_OF_CORE_KEYS\n",
    "\n",
    "# List 유형의 N개의 학습 데이터를 전달 받고, 정해진 컬럼에 대해서 정규화하여 반환하는 함수\n",
    "def apply_norm(x_sessions, keys_norm):\n",
    "    import pandas as pd\n",
    "    X0 = pd.DataFrame(x_sessions).values\n",
    "    X1 = X0[:,0:SIZE_OF_CORE_KEYS]      # 주요 킷값 \n",
    "    X2 = X0[:,SIZE_OF_CORE_KEYS:SIZE_OF_COEF_KEYS]    # 추가 킷값 -- normalize 대상\n",
    "    X3 = X0[:,SIZE_OF_COEF_KEYS:]        # 정답 라벨\n",
    "    x2_norm = normalize(X2, axis=0)\n",
    "    x_norm = np.concatenate((X1, x2_norm, X3), axis=1)\n",
    "    return x_norm\n",
    "\n",
    "# 최대 4개의 로그를 받아서 최종 출력 대상 튜플을 반환하는 함수\n",
    "# 적어도 2개 이상의 튜플이 있어야 증감을 측정할 수 있으므로 튜플이 하나이면 모든 값들을 그대로 쓰고, 증감 수치는 0으로 저장\n",
    "# 일반 필드의 값들은 평균치를 취하여 값으로 저장하도록 한다.\n",
    "# 기존의 값들은 그대로 사용하는 것이 좋을 것 같고 증감에 유의미한 필드에 대해서만 선정해서 필드를 추가하도록 수정\n",
    "def sessionize(sessions, training=False):\n",
    "    x_session = average_sessions(sessions, keys_sessions, training)\n",
    "    x_coef = extract_coef(sessions, keys_coef)\n",
    "    x_session.update(x_coef)\n",
    "    return x_session\n",
    "\n",
    "# 세션 목록을 디버깅 하기 위한 함수\n",
    "def debug_sessions(sessions):\n",
    "    for session in sessions:\n",
    "        for key in sorted(session.keys()):\n",
    "            value = session.get(key)\n",
    "            print(key, value)\n",
    "        print(session.get(\"actor_account_id\"), session.get(\"week\"), session.get(\"y_survival_time\"))\n",
    "        print(session)\n",
    "        \n",
    "def create_headers():\n",
    "    headers = keys_sessions + [ \"x_\" + key for key in keys_coef ] + [ \"y_\" + key for key in keys_labels ]\n",
    "    return headers\n",
    "\n",
    "# 생성된 dict 객체를 파일로 저장하는 함수\n",
    "def store_sessions(filename, sessions, headers):\n",
    "    with open(filename, 'w+') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(headers)\n",
    "        for session in sessions:\n",
    "           writer.writerow([ value for key, value in sorted(session.items()) ])\n",
    "        \n",
    "def store_nparray(filename, norm_array, headers):\n",
    "    with open(filename, 'w+') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(headers)\n",
    "        for norm in norm_array.tolist():\n",
    "            writer.writerow(norm)\n",
    "            \n",
    "            \n",
    "def main(source_data, target_data, threshold, training=False):\n",
    "    # 파일을 읽어서 동일한 actor_account_id 값을 하나로 묶고, actor 당, 출석일수, 행동의 coef 값을 반환하는 함수\n",
    "    items = dictionize(source_data)\n",
    "    prev_id = None\n",
    "    curr_id = None\n",
    "    sessions = []\n",
    "    x_sessions = []\n",
    "    lineno = 0\n",
    "    for item in items:\n",
    "        if lineno > threshold: break\n",
    "        curr_id = item.get(\"actor_account_id\")\n",
    "        if prev_id != None and prev_id != curr_id:\n",
    "            x_session = sessionize(sessions, training)\n",
    "            x_sessions.append(x_session)\n",
    "            sessions.clear()\n",
    "        prev_id = curr_id\n",
    "        sessions.append(item)\n",
    "        lineno += 1\n",
    "\n",
    "    if len(sessions) > 0:\n",
    "        x_session = sessionize(sessions, training)\n",
    "        x_sessions.append(x_session)\n",
    "\n",
    "    headers = create_headers()\n",
    "    y_sessions = apply_norm(x_sessions, keys_coef)\n",
    "    store_nparray(target_data, y_sessions, headers)\n",
    "    # store_sessions(target_data, x_sessions, headers)\n",
    "    \n",
    "\n",
    "\n",
    "def test_extract_coef(source):\n",
    "    items = dictionize(source)\n",
    "    sessions = []\n",
    "    x_sessions = []\n",
    "    x_coef = extract_coef(items, keys_coef)\n",
    "    print(x_coef)\n",
    "    \n",
    "\n",
    "__DEBUG__ = False\n",
    "__TRAIN__ = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threshold = sys.maxsize\n",
    "    \n",
    "    if __DEBUG__:\n",
    "        source=\"./source.csv\"\n",
    "        target=\"./target.csv\"\n",
    "        test_extract_coef(source)\n",
    "    elif __TRAIN__: # 학습용\n",
    "        source=\"data/train_data_sorted.csv\"\n",
    "        target=\"data/train_data_output.csv\"\n",
    "        main(source, target, threshold, True)\n",
    "    else:                # 테스트용\n",
    "        source=\"data/test_data_sorted.csv\"\n",
    "        target=\"data/test_data_output.csv\"\n",
    "        main(source, target, threshold, False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# list(dict) ==> np.array\n",
    "list_dict = [{\"x\":\"id_1\",\"a\":1000,\"b\":2,\"c\":0.01},{\"x\":\"id_2\",\"a\":2830,\"b\":28,\"c\":0.9},{\"x\":\"id_3\",\"a\":18293,\"b\":10,\"c\":0.0009}]\n",
    "np_array = np.asarray([[1000,2,0.01],[2830,28,0.9],[18293,10,0.0009]], dtype=np.float)\n",
    "\n",
    "pd_array = pd.DataFrame(list_dict).values # 영문 순서로 정렬 후 변환\n",
    "a_norm = normalize(pd_array[:, :3], axis=0)\n",
    "print(a_norm)\n",
    "x_norm = pd_array[:,[3]]\n",
    "pd_norm = np.concatenate(( pd_array[:,[3]], a_norm, x_norm), axis=1)\n",
    "print(pd_norm)\n",
    "np_norm = normalize(np_array, axis=0)\n",
    "print(np_norm)\n",
    "\n",
    "print(\"iterating array\")\n",
    "for x in np_norm.tolist():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "X = np.asarray([1, 2, 3, 4], dtype=np.float)\n",
    "X_train = X.reshape(4,1)\n",
    "Y = np.asarray([2,3,3,2], dtype=np.float)\n",
    "Y = preprocessing.normalize(Y, norm='l2')\n",
    "y_train = Y.reshape(4,1)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# print(Y, y_train)\n",
    "# print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(1000)*10\n",
    "print(x.shape)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys_intercepts = [ \"exp_get\", \"pvp_cnt\", \"quest_cnt\", \"inc_kina_sum\", \"enter_dd_cnt\", \"playtime_ss\", \"pve_cnt\", \"get_ap\", \"get_gp\",  \"fortress_cnt\", \"harvest_cnt\", \"combine_cnt\", \"use_scroll_cnt\" ]\n",
    "print(len(keys_intercepts))\n",
    "print(keys_intercepts[1])\n",
    "data = { \"x_\" + key:0.0 for key in keys_intercepts }\n",
    "for key in sorted(data.keys()):\n",
    "    value = data.get(key)\n",
    "    print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"a\", \"b\", \"c\"]\n",
    "y = [ {\"a\":\"1\", \"b\":\"10\", \"c\":\"100\"}, {\"a\":\"2\", \"b\":\"20\", \"c\":\"30\"}, {\"a\":\"3\", \"b\":\"30\", \"c\":\"300\"} ]\n",
    "num_of_y = len(y)\n",
    "import numpy as np\n",
    "for name in names:\n",
    "    Y = np.asarray([ y[i][name] for i in range(num_of_y) ], dtype=np.float)\n",
    "    Y = Y.reshape(num_of_y, 1)\n",
    "    print(Y)\n",
    "\n",
    "# Y = np.asarray([1.389234,8.389,19.233,0.8], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sessions = [ {\"x\":\"20000\", \"y\":\"10\", \"z\":\"100\"}, {\"a\":\"2\", \"b\":\"20\", \"c\":\"30\"}, {\"a\":\"3\", \"b\":\"30\", \"c\":\"300\"} ]\n",
    "for session in sessions:\n",
    "    print([[value] for key, value in sorted(session.items())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tf35]",
   "language": "python",
   "name": "Python [tf35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
