# 4장. 신경망 학습
신경망 학습은 입력 데이터와 출력 데이터를 알고 있는 상황에서, 어떠한 입력값에 대한 출력값을 예측하는 수많은 방정식의 결합을 추측하는 과정이라고 생각합니다.
그리고, 이러한 수많은 방정식의 매개변수를 추정 혹은 가장 근사한 값을 구하는 것이 신경망의 학습 과정이라고 말할 수 있습니다.


## 0.0 용어정리

### [방정식equation](https://ko.wikipedia.org/wiki/방정식)
> 식에 있는 특정한 문자의 값에 따라 참/거짓이 결정되는 등식을 말한다.
> 이때, 방정식을 참이 되게 하는 특정 문자의 값을 해 또는 근이라 한다.
> 방정식의 해는 없을 수도 있고, 여러 개일 수도 있고, 모든 값일 수도 있다. 전자의 경우는 불능이라고 하고, 중자의 경우는 방정식, 후자의 경우는 항등식(부정)이라 한다.
> 부정방정식은 해가 무수히 많지만, 항등식은 아니다

* 다항방정식
    * 일차방정식, 이차방정식, 삼차방정식, 고차방정식 등과 같이 미지수에 대한 다항식으로만 이루어진 방정식을 다항 방정식이라고 한다. 다항 방정식(多項方程式)은

* 고차방정식
    * 방정식의 미지수의 차수가 2차 이상인 경우를 말한다.
    * x^2 + x^3 ... x^n = 0

* 다차원방정식
    * 미지수의 가짓수가 2개 이상인 경우를 말한다. (아래의 경우는 일차 다차원 방정식이다)
    * w1\*x1 + w2\*x2 + ... wn\*xn = b1

### [행렬식determinent](https://ko.wikipedia.org/wiki/행렬식)
> 정사각행렬에 수를 대응시키는 함수의 하나이며, 대략, 정사각행렬이 나타내는 선형 변환이 부피를 확대시키는 정도를 나타낸다.
> 연립 선형방정식의 성질을 결정하기 위해 정의되었고, 행렬식의 영어 이름 "디터미넌트"(영어: determinant)는 "디터민"(영어: determine)(결정하다)에서 유래하였다
> 행렬식이 0이 아닌지 여부는 연립방정식이 유일한 해를 갖는지를 결정한다. 

### [매개변수parameter](https://ko.wikipedia.org/wiki/매개변수)
> 매개변수(媒介變數), 파라미터(parameter), 모수(母數)는 수학과 통계학에서 어떠한 시스템이나 함수의 특정한 성질을 나타내는 변수를 말한다. 일반적으로는 θ 라고 표현한다.

### [부동소수점floating point](https://ko.wikipedia.org/wiki/부동소수점)
> 부동소수점(浮動小數點, floating point) 또는 떠돌이 소수점 방식은 실수를 표현할 때 소수점의 위치를 고정하지 않고 그 위치를 나타내는 수를 따로 적는 것으로, 유효숫자를 나타내는 가수(假數)와 소수점의 위치를 풀이하는 지수(指數)로 나누어 표현한다.
* 실제 컴퓨터에서는 보통 이진법을 사용하여 밑수를 하고, 다음과 같이 세 부분의 값으로 실수를 나타낸다.
    * 부호부 (1비트)
    * 지수부 (부호가 있는 정수)
    * 가수부 (부호가 없는 정수)
* [부동 소수점 표현방식](https://winterj.me/Floating-Point)
    * Single Precision (4bytes): 32비트 체제(단일 정밀도 형식)에서는 부호 부분 1, 지수 부분 8, 가수 부분 23개로 비트를 할당합니다.
    * Double Precision (8bytes): 64비트 체제(이중 정밀도 형식)에서는 부호 부분 1, 지수 부분 11, 가수 부분 52개로 비트를 할당합니다.
* [컴퓨터에서의 수 표현](https://namu.wiki/w/컴퓨터에서의%20수%20표현)
    * 컴퓨터에서는 실수를 10진수로 표현하지 않고 2진수로 표현하며, 가수와 지수부분을 나누어 계산하기 때문에, 실제 계산과 다르게 정확하지 않으며, 근사값이라 볼 수 있다



## 4.1. 데이터를 통해 학습한다

### 훈련\_데이터training\_data, 시험\_데이터test\_data
* 훈련 데이터만을 이용해 학습 및 매개변수를 찾고, 시험 데이터를 사용하여 평가하는 것이 일반적인데 이는 범용 능력을 제대로 평가하기 위함이며 과적합overfitting을 피하기 위함이다.


## 4.2 손실함수 

### 4.2.2 교차 엔트로피 오차

### [엔트로피](http://blog.acronym.co.kr/433)란?
* 확률변수의 분포의 특징을 하나의 숫자로 표현하기 위한 방법 
* 어떤 확률변수의 불확실을 측정하는 수, Claude Shannon 은 정보량이라 표현했다
    * entropy = -1 * Sigma { p(x) * log2(p(x)) } 와 같이 곱의 합으로 계산
    * 확률은 0~1 사이의 값이며 log 취한 값은 음수로 나와서 결과값은 항상 양수가 나온다
    * 4x4 크기의 트럼프 카드를 놓아두었고, 임의의 질문을 통해 알아맞힐 수 있는 회수는 4회인데 2^4=16 이라고 표현된다.
    * 이는 4번의 질문으로 불확실성을 해결하였으므로, 엔트로피는 4라고 말할 수 있다
* 밑수가 2인 로그의 경우 2진수 표현에서 bit는 각 자리수를 말하므로 숫자 16을 표현할 수 있는 최소 자릿수.
* "어떤 랜덤변수가 8가지의 상태로 동일한 확률로 발생한다고 할 때에, 엔트로피는 무엇이고, 어떤 의미를 가지는가?"
    * 2 ^ 3 = 8 이며, 이는 3번으로 불확실성을 없앨 수 있고, 3bit로 모든 수를 표현할 수 있다
* "그러면, 8개의 확률변수가 등장할 각 각의 확률이 다른경우는 어떠하고, 어떤 경우에 더 높아지고, 낮아지는가?"
    * 균등한 경우보다 불균등한 경우가 더 엔트로피 값이 작고, 이는 더 쉽게 맞출 수 있고, 불확실성이 낮아졌다고 말할 수 있다

### 교차 엔트로피란?
* 두 확률변수의 상관관계를 하나의 숫자로 표현하기 위한 방법
* 엔트로피란, 불확실성이고 균등한 경우에 가장 높다라고 말했는데, 어떤 확률분포의 무작위성(randomness)를 말하며, 확률분포를 갖는 랜덤변수 X를 표현하기 위한 최소 비트수
    * log 값에 대해 확률밀도함수는 0~1사이의 값이므로 항상 log(x)는 음수가 나오며, -1을 곱하여 항상 양수값이 나오게 된다.
    * cross-entropy = -1\*Sigma{p(x)\*log2(m(x))} 이며  두 가지 확률분포가 동일한 경우에 최소값이 나온다
* 두 확률분포가 얼마나 가깝거나 혹은 먼지를 나타내는 값이다.
    * cost function 과 같이 기대값(확률변수)와 실제 연산값의 차이가 클 수록 큰 값이 나오고 항상 양수로 나온다
    * 좀 더 자세한 사항은 [Cross-Entropy](http://iew3.technion.ac.il/CE) 에서 확인이 가능하다


### 4.2.5 왜 손실 함수를 설정하는가?
* 신경망 학습에서는 최적의 매개변수(가중치 w와 편향 b)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾습니다.
    * 즉, 미분 값을 (기울기) 계산하고, 해당 미분값을 단서로 매개변수를 서서히 갱신하는 것
    * '가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변하나'라는 의미입니다.
    * 미분 값이 양수(+)이면 음의 방향으로 음수(-)이면 양의 방향으로 0이면 수렴했다고 보고 멈추게 됩니다.
* 신경망을 학습할 때 정확도를 지표로 삼아서는 안 되는데, 이는 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.
    * 예를 들어 100장 가운데 35장을 맞춘 모델이 있다면 35%의 정확률을 가지게 되며, 매개변수의 조정에 따라 정확률은 흔들리거나 멈출 수 있다
    * 실제로는 맞추는 대상은 달라지지만 갯수에 따라 정확률이 결정나기 때문에 미세한 차이를 가질 수 없다 1 단위로 불연속적으로 정확률이 변하기 때문이다
    * 반면, 손실함수의 경우는 소수점 이하의 값으로 나오게 되어 연속적인 값으로 비교가 가능하다.
    * 여기서 손실함수의 선택이 중요한다 계단함수의 경우는 대부분의 위치에서 기울기가 0이지만, sigmoid 함수의 경우는 기울기가 0인 지점이 없다.

### Neural Network 구성하는 과정은 무엇을 의미하는가?
* 결국 **입력값과 출력값이 정해진 상태에서 임의의 방정식의 매개변수를 구하는 과정**이며 이러한 방정식은 다양한 방법으로 구할 수 있다
* 단순한 2차 방정식을 통해 2개의 해를 구할 수도 있겠지만, 차수를 높이거나(노드의 수를 늘리는 경우), 변수를 늘리거나(노드의 깊이를 늘리는 경우) 할 수도 있다
    * 즉, 이러한 **방정식의 해를 구하는 과정을 matrix 연산을 통해 가능하며, 이를 network 형태로 표현한 것**이 neural network 이다

### 각 Layer를 통과한다는 의미는 무엇인가?
* 방정식의 매개변수를 모르기 때문에, 임의의 매개변수(w,b)를 **대충 때려 맞추는 과정**이다.

### Activation Function 은 어떤 역할을 하는가?
* 어차피 때려 맞춘 값이므로, 똑같은 가중치에 선형함수는 영향을 미치지 못 하므로, 랜덤한 가중치와, 비선형적인 함수를 통해서 랜덤하게 값에 변화를 주는 것 **매개변수의 값을 흔드는 과정**이다
* 주어진 확률변수를 단조증가 함수인 sigmoid 함수의 결과로 변환하는 과정이며, 학습이 진행함에 따라 sigmoid 곡선 상의 임의의 점을 왔다갔다 한다는 의미다

### 1회 Forward 과정을 거치는 것은 어떤 의미인가?
* **처음으로 매개변수를 구한 과정**이며, 추정한 1차 매개변수를 구한 상태이다. (졸라 복잡한 방정식이다)

### Cost Funciton 은 어떤 역할을 하는가?
* **대충 때려잡은 방정식이 얼마나 정확한 지를 판단하는 과정**이며, 여기서 다양한 비용함수(MSE, CEE)를 통해 구할 수 있다 

### 왜 미분이라는 과정을 거치는가?
* 방정식을 구했고, 이 **방정식의 매개변수는 2개(w,b)이므로 최종 출력값(해)와의 차이(오차)를 최소화** 하기 위해 편미분을 한다

### 배치용 교차 엔트로피 오차 구현하기
* [학습속도 문제와 Cross-Entropy Cost Function](http://laonple.blog.me/220554852626)


## 4.3 수치 미분

### 4.3.1 미분
* 속도 (거리/시간) = 단위 시간 당 달린 평균 거리를 말함
    * 단위 시간이 충분히 작은 시간인 경우(순간)의 변화량을 말함 (delta)
    * 미분(differentiation, derivative) = df(x)/dx = ( f(x+dx) - f(x) ) / dx
```python
    def numerical_diff(f, x):
        h = 10e-50
        return (f(x + h) - f(x)) / h
```
    * 여기서의 문제점은 부동소수점의 한계


## 4.4 기울기gradient
* 모든 변수에 대한 편미분을 벡터로 정리한 것
    * x0와 x1의 편미분을 동시에 계산하여, (df/dx0, df/dx1)과 같이 모든 변수의 편미분을 벡터로 정리한 것을 기울기gradient라고 합니다.
```python
    def numerical_gradient(f, x):
        h = 1e-4
        grad = np.zeros_like(x)
        for idx in range(x.size):
            tmp_value = x[idx]
            x[idx] = tmp_val + h
            fxh1 = f(x)
            x[idx] = tmp_val - h
            fxh2 = f(x)
            grad[idx] = (fxh1 - fxh2) / (2*h)
            x[idx] = tmp_val
        return grad
```

### 4.4.1 경사하강법
* 최적의 매개변수 즉 손실 함수가 최솟값이될 때의 매개변수 값을 말하는데, 일반적인 손실함수의 공간은 광대하여 어디가 최솟값이 되는지 알기 어렵습니다.
    * 이런 상황에서 기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법입니다
    * 함수가 극솟값(국소적인 최솟값), 최솟값, 또 안장점saddle point이 되는 장소에서는 기울기가 0입니다.
    * 경사법은 기울기가 0인 지점을 찾지만 그 장소가 반드시 최솟값이라 볼 수는 없으며, 경우에 따라 평평한 고원plateau, 플레토를 찾아 정체될 수 있습니다.
* 하이퍼파라미터hyper parameter, 초매개변수란?
    * 가중치, 편향과 같은 자동으로 획득되는 매개변수와 다르게, 사람이 직접 설정해야만 하는 매개변수를 말한다
    * 학습횟수, 학습비율 등을 말함


## 4.5 학습 알고리즘 구현하기
* 전제
* 1단계 - 미니배치
* 2단계 - 기울기 산출
* 3단계 - 매개변수 갱신
* 4단계 - 반복

### 4.5.3 시험 데이터로 평가하기
* 에폭epoch ?
    * 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당하며, 10,000개를 100개 미니배치로 100회 수행하면 이를 1에폭이라 한다.


## Q&A
* 말로만 듣던 sigmoid 함수를 통한 딥러닝의 깊이가 깊어지면 성능이 정말 떨어지는가?
> 2층에서 3층으로 늘리기만 했는데 초기 학습속도가 현저히 떨어진다
    * 왜 그러한 현상이 발생하는 지 구체적인 원인이 무엇인가? vanishing garedient 문제로 추정 (초기 학습율이 떨어지는 것으로 보아)
    * 그러면 RELU 함수를 통해서 해소가 되는가? relu 로 전환 시에 "sequence too large; cannot be greater than 32" 오류로 실패
> 층 수와 관계없이 학습비율을 높이니(0.1=>0.3) 95%에서 97%까지 성능이 올라감
* 입력값이 N개일 때에 히든노드가 N보다 작은 경우 깊이가 2인 경우에 신경망으로 해결할 수 없는가? 가능하지만 성능이 낮을 것이다.
    * 만일 그렇다면 깊이가 3인 경우는 노드수가 작아도 가능한가? 깊이가 깊혀도 정확도는 올라가나 학습시간이 오래 걸릴 것이며, 적절한 것이 제일 좋다.
    * 깊이가 깊은 것이 좋은지, 노드의 수가 많은 것이 좋은지 어떤 기준으로 판단하는가? 경험?
* 초기 가중치에 왜 0.01을 곱하는가?
    * [Weight Initialization](http://cs231n.github.io/neural-networks-2/#init) magic number 0.01 은 초기 gaussian random number 로 추출한 weight 값에 의해 학습에 대한 영향을 가능한 줄이기 위한 ad-hoc 한 값이다. 반면에 gradient 값이 줄어드는 효과를 주어 학습이 느려질 수도 있다고 합니다.
    * **초기화 값을 0으로 만드는 실수** 에 따르면 가능한 편중되지 않은 초기화를 원하며, 정규화는 되었으면 하고, 절반은 양수, 나머지는 음수이면 좋겠고 초기 값은 0에 가까워야 좋다고 생각할 수 있으나, 잘못된 생각인데 왜냐하면 모든 뉴런이 같은 값을 내어준다면 동일한 기울기를 출력하기 때문이다. 
    * 적절한 접근은 *Small random numbers* 인데 여전히 모든 가중치를 가능한 0에 가깝게 하기를 원하지만 위에서 기술한 대로, 0은 적절하지 않으므로 아주 작은 값을 취하되  *symmetry braking*할 수 있는 정도를 원하기 때문이다. ```W = 0.01 * np.random.randn(D,H)``` 와 같이 mean 값이 0인 가우시안 분포에서 샘플링을 수행한다. 앞에서 0.01 값을 곱하는 초기 가중치 값이 학습에 영향을 덜미치도록 하는 의도일 뿐이다. 또한 반드시 작은 숫자들이 반드시 더 나은 결과를 보여준다는 보장은 없는데 이는 학습 과정에서 기울기값이 충분히 작아지기 때문에 backpropagation 과정에서 gradient 값이 작아 지는 영향을 미칠 수 있기 때문이다.

