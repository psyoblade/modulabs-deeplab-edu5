# 5장. 오차역전파법
> 4장에서 매개변수의 기울기 계산을 위해 수치미분을 통해 구하였는데, 이는 매번 모든 변수에 대한 미분계산을 하기 때문에 성능이슈가 커지기 때문에, 매개변수의 기울기를 효율적으로 계산하기 위한 오차역전파법 backpropagation을 소개합니다. 오차역전파를 구하는 데에는 수치를 통한 계산방법과 계산그래프를 통한 방법이 있는데 계산그래프computational graph를 통한 방법이 직관적입니다.

## 5.0 본 장에서 필요로하는 수학적인 지식 정리

### 5.0.1 (편)미분
* 여러 함수의 미분에 대한 기본 지식
    * exp => exp
    * 1/x => -1/(x^2)
    * sin => cos
    * cos => -sin
* partial derivative
    * ∂  (round D): '라운드 디'라고 읽고, 편미분
    * ∇  (nabla): '나블라' 혹은 델이라고 읽으며, 벡터 미분
* multivariable chain rule
    * 다변수 연쇄 법칙 

### 5.0.2 곱규칙
* multiple rule

### 5.0.3 알고리즘
* devide and conquer
* dynamic programming


## 5.1 계산그래프

### 5.1.2 국소적 계산
> 네트워크의 층으로 내려가면서 각 레이어와의 종속성이 생길 수도 있겠지만, 그와 무관한게 해당 레이어에서만 계산이 가능한 상황을 국소적 계산이라 표현하고 있습니다. 여기서 잊지말아야 할 점은 forward 단계에서는 입력값 x가 아주 조금(d) 변화할 때에 출력값 L이 어떻게 편화하는 지에 관심이 있다는 것입니다. 즉, 합성합수가 아니라 single variable function 으로 보고 출력값 L에 대하여 x에 대한 미분을 하는 것이며, 입력값 x의 변화에 따라 출력값 L이 어떻게 변화하는 지가 dL/dx 이다.
* 그림 5-5, 100원짜리 사과가 220원으로 판매가 되는 경우에 입력값이 1원 상승했을 때에 최종 2.2배 만큼 영향을 준다고 말할 수 있다.


## 5.2 연쇄법칙
> 합성함수의 미분에 대한 성질이며, "함성함수의 미분은 함성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다." 즉, forward propagation 단계는 함수의 연속 즉, 합성함수라고 얘기할 수 있고, backward propagation 단계는 미분의 곱으로 말할 수 있다. 

### 5.2.2 연쇄법칙이란
* 결국 연쇄법칙에 따라 각 국소적 단계의 결과값을 계산 및 저장해 두었다가 역전파 단계에서 각 미분값들의 곱으로 backprop 계산을 손쉽게 할 수 있다는 것이 핵심이다.
* 일련의 과정에서 말할 수 있는 것은 x에 대한 L의 미분을 계산한 결과가 된다.
* **결국 원하는 바는 입력값에 대한 최종 출력값을 예측하는 복잡한 방정식을 찾고 싶은 것이고, 이것을 함수라고 얘기할 수도 있겠다.** 


## 5.3 역전파
> 위에서 말한 것 처럼, 아주 복잡한 함수를 추측하는 데에 있어서 사칙연산 뿐만 아니라 다양한 함수들을 사용한 합성함수가 활용되어도 이상하지 않을 것이며, 이러한 노드가 은닉층에 포함된다고 보면 된다. 그런데 왜 활성화 함수activation function라는 것이 들어가야만 하는지 궁금할텐데 이는 단순히 선형함수의 결합은 하나의 함수를 수행하는 것과 다르지 않고 이러한 선형적인 것은 선형회귀로 풀수 있는 문제가 되는 것이므로, 신경망을 사용할 이유가 없다. 결국, **현실적인 문제해결은 비선형이라는 가정이 있는 것이고 그러기 위해서는 비선형함수의 결합이라는 가정**이 있어야만 한다.

## 5.4 단순한 계층 구현하기
> 덧셈, 곱셈 계층에 대한 설명을 보았으나, 결국 입력값의 각 노드의 작은 변화에 따라 얼마나 변화하는 지에 대한 변화량(미분값)을 알아낼 수 있었다.


## 5.5 활성화 함수 계층 구현하기
> 앞서 말한대로 활성화 함수를 통해서 비선형 은닉층 함수를 구할 수 있는데 sigmoid 함수를 사용했더니 은닉층이 깊어질 수록 미분한 값들이 점점 작아져서 결국 학습에 시간이 많이 걸리는 문제가 생기게 되었고 이를 해결하기 위해서 ReLU(Rectified Linear Unit)이라는 활성화함수가 고안되었다.


## 5.6 Affine/Softmax 계층 구현하기
> 신경망의 순전파 때 수행하는 행렬의 내적을 기하학에서 **어파인 변환affine transformation**이라고 합니다.

